{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import sentencepiece as spm\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 with Max-Q Design'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi', 'this', 'is', 'my', 'dog', '.', 'His', 'name', 'is', 'Jone'],\n",
       " ['hello', 'I', 'am', 'John', '<>', '<>', '<>', '<>', '<>', '<>']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
    "        The paddings should be at the end of each sentence.\n",
    "    @param sents (list[list[str]]): list of sentences, where each sentence\n",
    "                                    is represented as a list of words\n",
    "    @param pad_token (str): padding token\n",
    "    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
    "        than the max length sentence are padded out with the pad_token, such that\n",
    "        each sentences in the batch now has equal length.\n",
    "    \"\"\"\n",
    "    sents_padded = []\n",
    "\n",
    "    ### YOUR CODE HERE (~6 Lines)\n",
    "    max_length = max([len(s) for s in sents])\n",
    "    sents_padded = [sentence + [pad_token]*(max_length-len(sentence)) for sentence in sents]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return sents_padded\n",
    "sents = [['Hi','this','is','my','dog',\".\",\"His\",\"name\",\"is\",\"Jone\"],[\"hello\",'I','am','John']]\n",
    "pad_token = '<>'\n",
    "pad_sents(sents, pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7831,  0.1088, -0.4200],\n",
       "        [-0.7239,  0.9402,  0.2234],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [-0.8828,  0.9722, -0.2331],\n",
       "        [ 0.0344, -1.2378, -0.2923],\n",
       "        [ 1.1123, -0.4968, -0.2691],\n",
       "        [ 0.4424,  0.9856, -0.3367],\n",
       "        [ 0.3591,  0.1401, -0.5762],\n",
       "        [-1.7043, -0.4807, -1.4128],\n",
       "        [ 0.2419,  0.4790,  0.0478]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3, padding_idx=2)\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8828,  0.9722, -0.2331],\n",
       "         [-0.7239,  0.9402,  0.2234],\n",
       "         [ 0.0344, -1.2378, -0.2923],\n",
       "         [ 1.1123, -0.4968, -0.2691],\n",
       "         [ 0.4424,  0.9856, -0.3367],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.LongTensor([[3,1,4,5,6,2]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "m= nn.Dropout(p=0.2)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5696, -1.6651,  2.1375],\n",
       "        [-0.0653, -1.5919, -1.0968]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7120, -0.0000,  2.6718],\n",
       "        [-0.0816, -1.9899, -1.3710]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 3)\n",
    "m = nn.Linear(3, 2)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1235, -1.0468,  0.2595],\n",
       "        [ 0.0243, -1.9398, -0.0212]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1749, -0.4994],\n",
       "        [-0.1314, -0.6735]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "from docopt import docopt\n",
    "# from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "import sacrebleu\n",
    "from nmt_model import Hypothesis, NMT\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "from utils import read_corpus, batch_iter\n",
    "from vocab import Vocab, VocabEntry\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "docopt() missing 1 required positional argument: 'doc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\MachineLearning\\a4\\student\\test.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/MachineLearning/a4/student/test.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m args \u001b[39m=\u001b[39m docopt()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/MachineLearning/a4/student/test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_data_src \u001b[39m=\u001b[39m read_corpus(args[\u001b[39m'\u001b[39m\u001b[39m--train-src\u001b[39m\u001b[39m'\u001b[39m], source\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m, vocab_size\u001b[39m=\u001b[39m\u001b[39m21000\u001b[39m)       \u001b[39m# EDIT: NEW VOCAB SIZE\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/MachineLearning/a4/student/test.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_data_tgt \u001b[39m=\u001b[39m read_corpus(args[\u001b[39m'\u001b[39m\u001b[39m--train-tgt\u001b[39m\u001b[39m'\u001b[39m], source\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtgt\u001b[39m\u001b[39m'\u001b[39m, vocab_size\u001b[39m=\u001b[39m\u001b[39m8000\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: docopt() missing 1 required positional argument: 'doc'"
     ]
    }
   ],
   "source": [
    "args = docopt(__doc__)\n",
    "train_data_src = read_corpus(args['--train-src'], source='src', vocab_size=21000)       # EDIT: NEW VOCAB SIZE\n",
    "train_data_tgt = read_corpus(args['--train-tgt'], source='tgt', vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
